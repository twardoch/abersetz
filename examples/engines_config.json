{
  "defaults": {
    "engine": "tr/google",
    "from_lang": "auto",
    "to_lang": "en",
    "chunk_size": 1200,
    "html_chunk_size": 1800
  },
  "credentials": {
    "openai": {"env": "OPENAI_API_KEY"},
    "anthropic": {"env": "ANTHROPIC_API_KEY"},
    "siliconflow": {"env": "SILICONFLOW_API_KEY"},
    "deepseek": {"env": "DEEPSEEK_API_KEY"},
    "groq": {"env": "GROQ_API_KEY"},
    "mistral": {"env": "MISTRAL_API_KEY"},
    "togetherai": {"env": "TOGETHERAI_API_KEY"},
    "deepinfra": {"env": "DEEPINFRA_API_KEY"},
    "fireworks": {"env": "FIREWORKS_API_KEY"},
    "cerebras": {"env": "CEREBRAS_API_KEY"},
    "xai": {"env": "XAI_API_KEY"},
    "gemini": {"env": "GOOGLE_API_KEY"},
    "openrouter": {"env": "OPENROUTER_API_KEY"}
  },
  "engines": {
    "hysf": {
      "chunk_size": 2400,
      "credential": {"name": "siliconflow"},
      "options": {
        "model": "tencent/Hunyuan-MT-7B",
        "base_url": "https://api.siliconflow.com/v1",
        "temperature": 0.3
      }
    },
    "ullm": {
      "chunk_size": 2400,
      "options": {
        "profiles": {
          "default": {
            "base_url": "https://api.siliconflow.com/v1",
            "model": "tencent/Hunyuan-MT-7B",
            "credential": {"name": "siliconflow"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {
              "role": "You are a professional translator. Preserve formatting and technical terms."
            }
          },
          "gpt4": {
            "base_url": "https://api.openai.com/v1",
            "model": "gpt-4-turbo-preview",
            "credential": {"name": "openai"},
            "temperature": 0.3,
            "max_input_tokens": 128000,
            "prolog": {
              "role": "You are an expert translator with deep cultural knowledge."
            }
          },
          "gpt4o": {
            "base_url": "https://api.openai.com/v1",
            "model": "gpt-4o",
            "credential": {"name": "openai"},
            "temperature": 0.2,
            "max_input_tokens": 128000,
            "prolog": {}
          },
          "claude3-opus": {
            "base_url": "https://api.anthropic.com/v1",
            "model": "claude-3-opus-20240229",
            "credential": {"name": "anthropic"},
            "temperature": 0.3,
            "max_input_tokens": 200000,
            "prolog": {
              "role": "You are Claude, an expert translator focused on accuracy and nuance."
            }
          },
          "claude3-sonnet": {
            "base_url": "https://api.anthropic.com/v1",
            "model": "claude-3-5-sonnet-20241022",
            "credential": {"name": "anthropic"},
            "temperature": 0.3,
            "max_input_tokens": 200000,
            "prolog": {}
          },
          "deepseek": {
            "base_url": "https://api.deepseek.com/v1",
            "model": "deepseek-chat",
            "credential": {"name": "deepseek"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {
              "note": "Optimized for Chinese and technical content"
            }
          },
          "groq-llama3": {
            "base_url": "https://api.groq.com/openai/v1",
            "model": "llama-3.1-70b-versatile",
            "credential": {"name": "groq"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "groq-mixtral": {
            "base_url": "https://api.groq.com/openai/v1",
            "model": "mixtral-8x7b-32768",
            "credential": {"name": "groq"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "mistral-large": {
            "base_url": "https://api.mistral.ai/v1",
            "model": "mistral-large-latest",
            "credential": {"name": "mistral"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "mistral-medium": {
            "base_url": "https://api.mistral.ai/v1",
            "model": "mistral-medium-latest",
            "credential": {"name": "mistral"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "together-llama": {
            "base_url": "https://api.together.xyz/v1",
            "model": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            "credential": {"name": "togetherai"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "together-mixtral": {
            "base_url": "https://api.together.xyz/v1",
            "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "credential": {"name": "togetherai"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "deepinfra-llama": {
            "base_url": "https://api.deepinfra.com/v1/openai",
            "model": "meta-llama/Meta-Llama-3.1-70B-Instruct",
            "credential": {"name": "deepinfra"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "deepinfra-qwen": {
            "base_url": "https://api.deepinfra.com/v1/openai",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "credential": {"name": "deepinfra"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {
              "note": "Excellent for Asian languages"
            }
          },
          "fireworks-llama": {
            "base_url": "https://api.fireworks.ai/inference/v1",
            "model": "accounts/fireworks/models/llama-v3p1-70b-instruct",
            "credential": {"name": "fireworks"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "cerebras-llama": {
            "base_url": "https://api.cerebras.ai/v1",
            "model": "llama3.1-70b",
            "credential": {"name": "cerebras"},
            "temperature": 0.3,
            "max_input_tokens": 8192,
            "prolog": {
              "note": "Very fast inference"
            }
          },
          "xai-grok": {
            "base_url": "https://api.x.ai/v1",
            "model": "grok-beta",
            "credential": {"name": "xai"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {}
          },
          "gemini-pro": {
            "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
            "model": "gemini-1.5-pro",
            "credential": {"name": "gemini"},
            "temperature": 0.3,
            "max_input_tokens": 1048576,
            "prolog": {
              "note": "Supports extremely long context"
            }
          },
          "gemini-flash": {
            "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
            "model": "gemini-1.5-flash",
            "credential": {"name": "gemini"},
            "temperature": 0.3,
            "max_input_tokens": 1048576,
            "prolog": {
              "note": "Fast and cost-effective"
            }
          },
          "openrouter-auto": {
            "base_url": "https://openrouter.ai/api/v1",
            "model": "openrouter/auto",
            "credential": {"name": "openrouter"},
            "temperature": 0.3,
            "max_input_tokens": 32000,
            "prolog": {
              "note": "Automatically routes to best available model"
            }
          }
        }
      }
    }
  }
}
